# Website Scraper & Analyzer (Python)

## üìå Project Description__

This project is a **Python-based Web Scraping and Analysis Tool** designed to automatically search, scrape, and analyze website content based on a given keyword query. The project supports two operational modes:

1. **Online Mode (OpenAI API)** ‚Äì Uses AI embeddings for advanced text analysis.
2. **Offline Mode (TF-IDF)** ‚Äì Uses traditional keyword extraction for analysis without relying on API credits.

All results are **exported into an Excel file**, providing a structured, easy-to-read report with top keywords and word counts from the scraped pages.

---

## üìÇ Directory Structure

```
website_scraper/
‚îÇ
‚îú‚îÄ‚îÄ config.py           # Configuration variables
‚îú‚îÄ‚îÄ search_web.py       # Finds related websites from Google Search
‚îú‚îÄ‚îÄ scraper.py          # Extracts text from target websites
‚îú‚îÄ‚îÄ chunker.py          # Splits text into chunks for processing
‚îú‚îÄ‚îÄ embedder.py         # Handles embeddings (OpenAI API)
‚îú‚îÄ‚îÄ analyzer.py         # Analyzes text using TF-IDF or AI embeddings
‚îú‚îÄ‚îÄ database_manager.py # Manages FAISS database for vector storage
‚îú‚îÄ‚îÄ online_main.py      # Runs full pipeline using OpenAI API
‚îú‚îÄ‚îÄ offline_main.py     # Runs full pipeline using TF-IDF analysis
‚îú‚îÄ‚îÄ main.py             # Auto-detects mode (online/offline)
‚îú‚îÄ‚îÄ requirements.txt    # Required Python packages
‚îî‚îÄ‚îÄ README.md           # Documentation
```

---

## ‚öôÔ∏è Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/website_scraper.git
cd website_scraper
```

2. Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # (Linux/Mac)
venv\Scripts\activate     # (Windows)
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. (Optional) Set up your OpenAI API key:

```bash
echo "OPENAI_API_KEY=your_key_here" > .env
```

If no API key is found, the project automatically switches to **offline mode**.

---

## üöÄ Usage

Run the project:

```bash
python main.py
```

### Workflow:

1. Enter a search keyword (e.g., `AI in manufacturing industry`).
2. The system retrieves relevant websites via Google Search.
3. Each website is scraped for text content.
4. Text data is split into smaller chunks for analysis.
5. **Online Mode** ‚Üí Uses OpenAI embeddings to process text.
6. **Offline Mode** ‚Üí Uses TF-IDF for keyword extraction.
7. Results are saved into `output_websites.xlsx`.

---

## üìä Output Example

| URL                                                                    | Word Count | Top Keywords                  |
| ---------------------------------------------------------------------- | ---------- | ----------------------------- |
| [https://www.ibm.com/think/topics](https://www.ibm.com/think/topics)   | 1584       | ai, manufacturing, automation |
| [https://www.autodesk.com/articles](https://www.autodesk.com/articles) | 1342       | robotics, ai, industry        |

The Excel file will be generated automatically in the project folder.

---

## üõ† Technical Documentation

### 1Ô∏è‚É£ `search_web.py`

* Uses `googlesearch` or `requests` to fetch top websites related to the keyword.
* Configurable for number of results and language.

### 2Ô∏è‚É£ `scraper.py`

* Extracts visible text from `<p>` tags.
* Cleans whitespace and HTML artifacts.

### 3Ô∏è‚É£ `chunker.py`

* Splits long text into smaller, manageable pieces (default: 300 words).

### 4Ô∏è‚É£ `embedder.py`

* Generates text embeddings using OpenAI API (`text-embedding-ada-002`).
* Used only in online mode.

### 5Ô∏è‚É£ `analyzer.py`

* Offline: Uses TF-IDF (`sklearn`) to extract top keywords.
* Online: Uses semantic similarity and embeddings for clustering keywords.

### 6Ô∏è‚É£ `database_manager.py`

* Stores embeddings in a FAISS vector database for similarity searches.

### 7Ô∏è‚É£ `main.py`

* Auto-selects mode based on presence of `OPENAI_API_KEY`.
* Handles pipeline execution and Excel export.

---

## üìú License

This project is licensed under the MIT License ‚Äì you are free to use, modify, and distribute it with proper attribution.

---

## ü§ù Contributing

Pull requests are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature-name`)
3. Commit changes (`git commit -m 'Add feature'`)
4. Push to branch (`git push origin feature-name`)
5. Submit a pull request

---

## üí° Future Enhancements

* Add multi-language support for text analysis
* Implement content summarization per website
* Build a simple web UI for non-technical users
